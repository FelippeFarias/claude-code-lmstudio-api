# 要件書

## 概要

Claude Code SDKをバックエンドとしてAnthropicのサブスクリプションで使い放題にできるOpenAI API互換サーバーを作成します。公式のClaude Code SDK (https://docs.anthropic.com/ja/docs/claude-code/sdk) を使用してClaude Codeの機能にアクセスします。Docker Composeで起動でき、Claude Codeがローカルファイルを書き換えてもホストOSに影響が出ないよう隔離された環境で動作します。次点の目標として、LM Studio APIやOllama APIとの互換性も持たせます。

## 要件

### 要件 1

**ユーザーストーリー:** 開発者として、OpenAI APIクライアントを使ってClaude Codeの機能にアクセスしたい。そうすることで、既存のOpenAI APIベースのアプリケーションを変更せずにClaude Codeを利用できる。

#### 受け入れ基準

1. WHEN OpenAI API形式のリクエストを受信した時、THEN システムはClaude Code SDKを使用して適切に処理する
2. WHEN Claude Code SDKからレスポンスを受信した時、THEN システムはOpenAI API形式に変換してクライアントに返す
3. WHEN `/v1/chat/completions` エンドポイントにリクエストが送信された時、THEN システムはClaude Code SDKのチャット機能を呼び出す
4. WHEN `/v1/models` エンドポイントにリクエストが送信された時、THEN システムは利用可能なClaude Codeモデルのリストを返す

### 要件 2

**ユーザーストーリー:** システム管理者として、Docker Composeで簡単にサーバーを起動したい。そうすることで、複雑な環境構築なしに迅速にサービスを開始できる。

#### 受け入れ基準

1. WHEN `docker-compose up` コマンドを実行した時、THEN システムはポート1235でAPIサーバーを起動する
2. WHEN サーバーが起動した時、THEN システムはホストOSの1235ポートでリクエストを受け付ける
3. WHEN Docker Composeファイルが提供された時、THEN 必要な環境変数とボリュームマウントが適切に設定されている
4. WHEN サーバーが正常に起動した時、THEN ヘルスチェックエンドポイントが応答する

### 要件 3

**ユーザーストーリー:** セキュリティ担当者として、Claude Codeがローカルファイルを変更してもホストOSに影響が出ないようにしたい。そうすることで、安全にサービスを運用できる。

#### 受け入れ基準

1. WHEN Claude Codeがファイル操作を実行した時、THEN 操作はコンテナ内の隔離された環境でのみ実行される
2. WHEN コンテナが削除された時、THEN ホストOSのファイルシステムには変更が残らない
3. WHEN ファイル書き込み操作が発生した時、THEN 操作は専用のボリューム内でのみ実行される
4. IF ホストOSへのアクセスが試行された場合、THEN システムはアクセスを拒否する

### 要件 4

**ユーザーストーリー:** 開発者として、Anthropicのサブスクリプションを使って無制限にAPIを利用したい。そうすることで、使用量を気にせずに開発作業を進められる。

#### 受け入れ基準

1. WHEN APIリクエストが送信された時、THEN システムはAnthropic APIキーを使用してClaude Code SDKを通じてアクセスする
2. WHEN 複数のリクエストが同時に送信された時、THEN システムは適切にレート制限を管理する
3. WHEN Anthropic APIキーが設定された時、THEN システムは認証情報を安全に管理する
4. WHEN サブスクリプション制限に達した場合、THEN システムは適切なエラーメッセージを返す

### 要件 5

**ユーザーストーリー:** 開発者として、将来的にLM Studio APIやOllama APIとも互換性を持たせたい。そうすることで、複数のAIサービスを統一的に利用できる。

#### 受け入れ基準

1. WHEN システムアーキテクチャが設計された時、THEN 複数のバックエンドAPIをサポートできる拡張可能な構造になっている
2. WHEN 設定ファイルでバックエンドが指定された時、THEN システムは適切なAPIプロバイダーに接続する
3. WHEN LM Studio API形式のリクエストが送信された時、THEN システムは適切に処理できる（将来実装）
4. WHEN Ollama API形式のリクエストが送信された時、THEN システムは適切に処理できる（将来実装）

### 要件 6

**ユーザーストーリー:** 開発者として、APIの動作状況を監視したい。そうすることで、問題が発生した際に迅速に対応できる。

#### 受け入れ基準

1. WHEN APIリクエストが処理された時、THEN システムは適切なログを出力する
2. WHEN エラーが発生した時、THEN システムは詳細なエラー情報をログに記録する
3. WHEN ヘルスチェックが実行された時、THEN システムは現在の状態を返す
4. WHEN メトリクス情報が要求された時、THEN システムは基本的な統計情報を提供する