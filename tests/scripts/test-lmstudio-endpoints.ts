#!/usr/bin/env ts-node

/**
 * LM Studio API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ã‚¿ãƒ¼
 * v0 APIï¼ˆLM Studioç‹¬è‡ªï¼‰ã¨v1 APIï¼ˆOpenAIäº’æ›ï¼‰ã®ä¸¡æ–¹ã‚’ãƒ†ã‚¹ãƒˆ
 */

interface TestResult {
  endpoint: string
  method: string
  request?: any
  response?: any
  error?: any
  headers?: Record<string, string>
  streamingData?: string[]
}

const BASE_URL = process.env.LM_STUDIO_URL || 'http://localhost:1234'
const results: TestResult[] = []

// ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
async function testEndpoint(
  method: 'GET' | 'POST',
  endpoint: string,
  body?: any,
  streaming = false,
): Promise<TestResult> {
  const url = `${BASE_URL}${endpoint}`
  const result: TestResult = {
    endpoint,
    method,
    request: body,
  }

  try {
    console.log(`\nğŸ”„ Testing ${method} ${endpoint}...`)

    const options: RequestInit = {
      method,
      headers: {
        'Content-Type': 'application/json',
      },
    }

    if (body) {
      options.body = JSON.stringify(body)
    }

    const response = await fetch(url, options)

    // ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²
    result.headers = {}
    response.headers.forEach((value, key) => {
      result.headers![key] = value
    })

    if (streaming && response.headers.get('content-type')?.includes('text/event-stream')) {
      // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‡¦ç†
      const reader = response.body?.getReader()
      const decoder = new TextDecoder()
      const streamData: string[] = []

      if (reader) {
        try {
          while (true) {
            const { done, value } = await reader.read()
            if (done) break

            const chunk = decoder.decode(value)
            streamData.push(chunk)

            // æœ€åˆã®5ãƒãƒ£ãƒ³ã‚¯ã ã‘è¨˜éŒ²ï¼ˆå…¨éƒ¨ã ã¨é•·ã™ãã‚‹ï¼‰
            if (streamData.length >= 5) {
              streamData.push('... (truncated)')
              break
            }
          }
        } finally {
          reader.releaseLock()
        }
      }

      result.streamingData = streamData
      result.response = 'Streaming response - see streamingData'
    } else {
      // é€šå¸¸ã®JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹
      const text = await response.text()
      try {
        result.response = JSON.parse(text)
      } catch {
        result.response = text
      }
    }

    if (!response.ok) {
      result.error = `HTTP ${response.status}: ${response.statusText}`
    }
  } catch (error) {
    result.error = error instanceof Error ? error.message : String(error)
    console.error(`âŒ Error: ${result.error}`)
  }

  return result
}

// ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async function runTests() {
  console.log('ğŸš€ LM Studio API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆé–‹å§‹')
  console.log(`ğŸ“ Base URL: ${BASE_URL}`)

  // ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  const chatMessages = [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Say hello in one sentence.' },
  ]

  // ===============================
  // v0 API ãƒ†ã‚¹ãƒˆï¼ˆLM Studioç‹¬è‡ªï¼‰
  // ===============================
  console.log('\n\n=== v0 API (LM Studio Native) ===')

  // ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
  results.push(await testEndpoint('GET', '/api/v0/models'))

  // ãƒãƒ£ãƒƒãƒˆè£œå®Œï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
  results.push(
    await testEndpoint('POST', '/api/v0/chat/completions', {
      model: 'test-model',
      messages: chatMessages,
      temperature: 0.7,
      max_tokens: 50,
      stream: false,
    }),
  )

  // ãƒãƒ£ãƒƒãƒˆè£œå®Œï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
  results.push(
    await testEndpoint(
      'POST',
      '/api/v0/chat/completions',
      {
        model: 'test-model',
        messages: chatMessages,
        temperature: 0.7,
        max_tokens: 50,
        stream: true,
      },
      true,
    ),
  )

  // ãƒ†ã‚­ã‚¹ãƒˆè£œå®Œ
  results.push(
    await testEndpoint('POST', '/api/v0/completions', {
      model: 'test-model',
      prompt: 'Once upon a time',
      temperature: 0.7,
      max_tokens: 50,
      stream: false,
    }),
  )

  // åŸ‹ã‚è¾¼ã¿
  results.push(
    await testEndpoint('POST', '/api/v0/embeddings', {
      model: 'test-model',
      input: 'Hello world',
    }),
  )

  // ===============================
  // v1 API ãƒ†ã‚¹ãƒˆï¼ˆOpenAIäº’æ›ï¼‰
  // ===============================
  console.log('\n\n=== v1 API (OpenAI Compatible) ===')

  // ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
  results.push(await testEndpoint('GET', '/v1/models'))

  // ãƒãƒ£ãƒƒãƒˆè£œå®Œï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
  results.push(
    await testEndpoint('POST', '/v1/chat/completions', {
      model: 'test-model',
      messages: chatMessages,
      temperature: 0.7,
      max_tokens: 50,
      stream: false,
    }),
  )

  // ãƒãƒ£ãƒƒãƒˆè£œå®Œï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
  results.push(
    await testEndpoint(
      'POST',
      '/v1/chat/completions',
      {
        model: 'test-model',
        messages: chatMessages,
        temperature: 0.7,
        max_tokens: 50,
        stream: true,
      },
      true,
    ),
  )

  // ãƒ†ã‚­ã‚¹ãƒˆè£œå®Œ
  results.push(
    await testEndpoint('POST', '/v1/completions', {
      model: 'test-model',
      prompt: 'Once upon a time',
      temperature: 0.7,
      max_tokens: 50,
      stream: false,
    }),
  )

  // åŸ‹ã‚è¾¼ã¿
  results.push(
    await testEndpoint('POST', '/v1/embeddings', {
      model: 'test-model',
      input: 'Hello world',
    }),
  )

  // ===============================
  // ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ
  // ===============================
  console.log('\n\n=== Error Cases ===')

  // ç„¡åŠ¹ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
  results.push(await testEndpoint('GET', '/api/v0/invalid-endpoint'))

  // ç„¡åŠ¹ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£
  results.push(
    await testEndpoint('POST', '/v1/chat/completions', {
      // modelãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ¬ è½
      messages: chatMessages,
    }),
  )

  // çµæœã‚’å‡ºåŠ›
  outputResults()
}

// çµæœã®æ•´å½¢ã¨å‡ºåŠ›
function outputResults() {
  console.log(`\n\n${'='.repeat(80)}`)
  console.log('ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼')
  console.log('='.repeat(80))

  for (const result of results) {
    console.log(`\n### ${result.method} ${result.endpoint}`)

    if (result.request) {
      console.log('\nğŸ“¤ Request:')
      console.log(JSON.stringify(result.request, null, 2))
    }

    if (result.headers) {
      console.log('\nğŸ“‹ Response Headers:')
      console.log(JSON.stringify(result.headers, null, 2))
    }

    if (result.streamingData) {
      console.log('\nğŸ“¡ Streaming Data (first 5 chunks):')
      result.streamingData.forEach((chunk, i) => {
        console.log(`Chunk ${i + 1}: ${chunk.trim()}`)
      })
    } else if (result.response) {
      console.log('\nğŸ“¥ Response:')
      console.log(JSON.stringify(result.response, null, 2))
    }

    if (result.error) {
      console.log('\nâŒ Error:')
      console.log(result.error)
    }

    console.log(`\n${'-'.repeat(40)}`)
  }

  // ã‚¹ã‚­ãƒ¼ãƒã‚µãƒãƒªãƒ¼
  generateSchemaSummary()
}

// ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ã‚­ãƒ¼ãƒã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
function generateSchemaSummary() {
  console.log(`\n\n${'='.repeat(80)}`)
  console.log('ğŸ“‹ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ã‚­ãƒ¼ãƒã‚µãƒãƒªãƒ¼')
  console.log('='.repeat(80))

  const schemas: Record<string, any> = {}

  for (const result of results) {
    if (result.response && typeof result.response === 'object') {
      const key = `${result.method} ${result.endpoint}`
      schemas[key] = {
        fields: Object.keys(result.response),
        example: result.response,
      }
    }
  }

  console.log(JSON.stringify(schemas, null, 2))
}

// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if (require.main === module) {
  runTests().catch(console.error)
}

export { runTests, testEndpoint }
